:orphan:

:py:mod:`trieste.models.model_interfaces`
=========================================

.. py:module:: trieste.models.model_interfaces


Module Contents
---------------

.. py:class:: ProbabilisticModel

   Bases: :py:obj:`abc.ABC`

   A probabilistic model.

   .. py:method:: predict(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]
      :abstractmethod:

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points``.

      This is essentially a convenience method for :meth:`predict_joint`, where non-event
      dimensions of ``query_points`` are all interpreted as broadcasting dimensions instead of
      batch dimensions, and the covariance is squeezed to remove redundant nesting.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: predict_joint(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]
      :abstractmethod:

      :param query_points: The points at which to make predictions, of shape [..., B, D].
      :return: The mean and covariance of the joint marginal distribution at each batch of points
          in ``query_points``. For a predictive distribution with event shape E, the mean will
          have shape [..., B] + E, and the covariance shape [...] + E + [B, B].


   .. py:method:: sample(self, query_points: trieste.type.TensorType, num_samples: int) -> trieste.type.TensorType
      :abstractmethod:

      Return ``num_samples`` samples from the independent marginal distributions at
      ``query_points``.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: predict_y(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]
      :abstractmethod:

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: get_observation_noise(self) -> trieste.type.TensorType
      :abstractmethod:

      Return the variance of observation noise.

      Note that this is not supported by all models.

      :return: The observation noise.


   .. py:method:: get_kernel(self) -> gpflow.kernels.Kernel
      :abstractmethod:

      Return the kernel of the model.

      :return: The kernel.



.. py:class:: TrainableProbabilisticModel

   Bases: :py:obj:`ProbabilisticModel`

   A trainable probabilistic model.

   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None
      :abstractmethod:

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None
      :abstractmethod:

      Optimize the model objective with respect to (hyper)parameters given the specified
      ``dataset``.

      :param dataset: The data with which to train the model.



.. py:class:: ModelStack(model_with_event_size: tuple[TrainableProbabilisticModel, int], *models_with_event_sizes: tuple[TrainableProbabilisticModel, int])

   Bases: :py:obj:`TrainableProbabilisticModel`

   A :class:`ModelStack` is a wrapper around a number of :class:`TrainableProbabilisticModel`\ s.
   It combines the outputs of each model for predictions and sampling, and delegates training data
   to each model for updates and optimization.

   **Note:** Only supports vector outputs (i.e. with event shape [E]). Outputs for any two models
   are assumed independent. Each model may itself be single- or multi-output, and any one
   multi-output model may have dependence between its outputs. When we speak of *event size* in
   this class, we mean the output dimension for a given :class:`TrainableProbabilisticModel`,
   whether that is the :class:`ModelStack` itself, or one of the subsidiary
   :class:`TrainableProbabilisticModel`\ s within the :class:`ModelStack`. Of course, the event
   size for a :class:`ModelStack` will be the sum of the event sizes of each subsidiary model.

   The order of individual models specified at :meth:`__init__` determines the order of the
   :class:`ModelStack` output dimensions.

   :param model_with_event_size: The first model, and the size of its output events.
       **Note:** This is a separate parameter to ``models_with_event_sizes`` simply so that the
       method signature requires at least one model. It is not treated specially.
   :param \*models_with_event_sizes: The other models, and sizes of their output events.

   .. py:method:: predict(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The predictions from all the wrapped models, concatenated along the event axis in
          the same order as they appear in :meth:`__init__`. If the wrapped models have predictive
          distributions with event shapes [:math:`E_i`], the mean and variance will both have
          shape [..., :math:`\sum_i E_i`].


   .. py:method:: predict_joint(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]

      :param query_points: The points at which to make predictions, of shape [..., B, D].
      :return: The predictions from all the wrapped models, concatenated along the event axis in
          the same order as they appear in :meth:`__init__`. If the wrapped models have predictive
          distributions with event shapes [:math:`E_i`], the mean will have shape
          [..., B, :math:`\sum_i E_i`], and the covariance shape
          [..., :math:`\sum_i E_i`, B, B].


   .. py:method:: sample(self, query_points: trieste.type.TensorType, num_samples: int) -> trieste.type.TensorType

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples from all the wrapped models, concatenated along the event axis. For
          wrapped models with predictive distributions with event shapes [:math:`E_i`], this has
          shape [..., S, N, :math:`\sum_i E_i`], where S is the number of samples.


   .. py:method:: predict_y(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The predictions from all the wrapped models, concatenated along the event axis in
          the same order as they appear in :meth:`__init__`. If the wrapped models have predictive
          distributions with event shapes [:math:`E_i`], the mean and variance will both have
          shape [..., :math:`\sum_i E_i`].
      :raise NotImplementedError: If any of the models don't implement predict_y.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update all the wrapped models on their corresponding data. The data for each model is
      extracted by splitting the observations in ``dataset`` along the event axis according to the
      event sizes specified at :meth:`__init__`.

      :param dataset: The query points and observations for *all* the wrapped models.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize all the wrapped models on their corresponding data. The data for each model is
      extracted by splitting the observations in ``dataset`` along the event axis according to the
      event sizes specified at :meth:`__init__`.

      :param dataset: The query points and observations for *all* the wrapped models.



.. py:data:: M
   

   A type variable bound to :class:`tf.Module`. 


.. py:function:: module_deepcopy(self: M, memo: dict[int, object]) -> M

   This function provides a workaround for `a bug`_ in TensorFlow Probability (fixed in `version
   0.12`_) where a :class:`tf.Module` cannot be deep-copied if it has
   :class:`tfp.bijectors.Bijector` instances on it. The function can be used to directly copy an
   object ``self`` as e.g. ``module_deepcopy(self, {})``, but it is perhaps more useful as an
   implemention for :meth:`__deepcopy__` on classes, where it can be used as follows:

   .. _a bug: https://github.com/tensorflow/probability/issues/547
   .. _version 0.12: https://github.com/tensorflow/probability/releases/tag/v0.12.1

   .. testsetup:: *

       >>> import tensorflow_probability as tfp

   >>> class Foo(tf.Module):
   ...     example_bijector = tfp.bijectors.Exp()
   ...
   ...     __deepcopy__ = module_deepcopy

   Classes with this method can be deep-copied even if they contain
   :class:`tfp.bijectors.Bijector`\ s.

   :param self: The object to copy.
   :param memo: References to existing deep-copied objects (by object :func:`id`).
   :return: A deep-copy of ``self``.


.. py:class:: GPflowPredictor(optimizer: Optimizer | None = None)

   Bases: :py:obj:`ProbabilisticModel`, :py:obj:`tensorflow.Module`, :py:obj:`abc.ABC`

   A trainable wrapper for a GPflow Gaussian process model.

   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.

   .. py:method:: optimizer(self) -> trieste.models.optimizer.Optimizer
      :property:

      The optimizer with which to train the model.


   .. py:method:: model(self) -> gpflow.models.GPModel
      :property:

      The underlying GPflow model.


   .. py:method:: predict(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points``.

      This is essentially a convenience method for :meth:`predict_joint`, where non-event
      dimensions of ``query_points`` are all interpreted as broadcasting dimensions instead of
      batch dimensions, and the covariance is squeezed to remove redundant nesting.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: predict_joint(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]

      :param query_points: The points at which to make predictions, of shape [..., B, D].
      :return: The mean and covariance of the joint marginal distribution at each batch of points
          in ``query_points``. For a predictive distribution with event shape E, the mean will
          have shape [..., B] + E, and the covariance shape [...] + E + [B, B].


   .. py:method:: sample(self, query_points: trieste.type.TensorType, num_samples: int) -> trieste.type.TensorType

      Return ``num_samples`` samples from the independent marginal distributions at
      ``query_points``.

      :param query_points: The points at which to sample, with shape [..., N, D].
      :param num_samples: The number of samples at each point.
      :return: The samples. For a predictive distribution with event shape E, this has shape
          [..., S, N] + E, where S is the number of samples.


   .. py:method:: predict_y(self, query_points: trieste.type.TensorType) -> tuple[trieste.type.TensorType, trieste.type.TensorType]

      Return the mean and variance of the independent marginal distributions at each point in
      ``query_points`` for the observations, including noise contributions.

      Note that this is not supported by all models.

      :param query_points: The points at which to make predictions, of shape [..., D].
      :return: The mean and variance of the independent marginal distributions at each point in
          ``query_points``. For a predictive distribution with event shape E, the mean and
          variance will both have shape [...] + E.


   .. py:method:: get_kernel(self) -> gpflow.kernels.Kernel

      Return the kernel of the model.

      :return: The kernel.


   .. py:method:: get_observation_noise(self)

      Return the variance of observation noise for homoscedastic likelihoods.
      :return: The observation noise.
      :raise NotImplementedError: If the model does not have a homoscedastic likelihood.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      :param dataset: The data with which to optimize the `model`.



.. py:class:: GaussianProcessRegression(model: GPR | SGPR, optimizer: Optimizer | None = None, num_kernel_samples: int = 10)

   Bases: :py:obj:`GPflowPredictor`, :py:obj:`TrainableProbabilisticModel`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.GPR`
   or :class:`~gpflow.models.SGPR`.

   :param model: The GPflow model to wrap.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param num_kernel_samples: Number of randomly sampled kernels (for each kernel parameter) to
       evaluate before beginning model optimization. Therefore, for a kernel with `p`
       (vector-valued) parameters, we evaluate `p * num_kernel_samples` kernels.

   .. py:method:: model(self) -> GPR | SGPR
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.


   .. py:method:: covariance_between_points(self, query_points_1: trieste.type.TensorType, query_points_2: trieste.type.TensorType) -> trieste.type.TensorType

      Compute the posterior covariance between sets of query points.

      .. math:: \Sigma_{12} = K_{12} - K_{x1}(K_{xx} + \sigma^2 I)^{-1}K_{x2}

      :param query_points_1: Set of query points with shape [N, D]
      :param query_points_2: Sets of query points with shape [M, D]

      :return: Covariance matrix between the sets of query points with shape [N, M]


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      Optimize the model with the specified `dataset`.

      For :class:`GaussianProcessRegression`, we (optionally) try multiple randomly sampled
      kernel parameter configurations as well as the configuration specified when initializing
      the kernel. The best configuration is used as the starting point for model optimization.

      For trainable parameters constrained to lie in a finite interval (through a sigmoid
      bijector), we begin model optimization from the best of a random sample from these
      parameters' acceptable domains.

      For trainable parameters without constraints but with priors, we begin model optimization
      from the best of a random sample from these parameters' priors.

      For trainable parameters with neither priors nor constraints, we begin optimization from
      their initial values.

      :param dataset: The data with which to optimize the `model`.


   .. py:method:: find_best_model_initialization(self, num_kernel_samples) -> None

      Test `num_kernel_samples` models with sampled kernel parameters. The model's kernel
      parameters are then set to the sample achieving maximal likelihood.

      :param num_kernel_samples: Number of randomly sampled kernels to evaluate.



.. py:class:: SparseVariational(model: gpflow.models.SVGP, data: trieste.data.Dataset, optimizer: Optimizer | None = None)

   Bases: :py:obj:`GPflowPredictor`, :py:obj:`TrainableProbabilisticModel`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.SVGP`.

   :param model: The underlying GPflow sparse variational model.
   :param data: The initial training data.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.

   .. py:method:: model(self) -> gpflow.models.SVGP
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.



.. py:class:: VariationalGaussianProcess(model: gpflow.models.VGP, optimizer: Optimizer | None = None, use_natgrads: bool = False, natgrad_gamma: Optional[float] = None)

   Bases: :py:obj:`GPflowPredictor`, :py:obj:`TrainableProbabilisticModel`

   A :class:`TrainableProbabilisticModel` wrapper for a GPflow :class:`~gpflow.models.VGP`.

   A Variational Gaussian Process (VGP) approximates the posterior of a GP
   using the multivariate Gaussian closest to the posterior of the GP by minimizing the
   KL divergence between approximated and exact posteriors. See :cite:`opper2009variational`
   for details.

   The VGP provides (approximate) GP modelling under non-Gaussian likelihoods, for example
   when fitting a classification model over binary data.

   A whitened representation and (optional) natural gradient steps are used to aid
   model optimization.

   :param model: The GPflow :class:`~gpflow.models.VGP`.
   :param optimizer: The optimizer with which to train the model. Defaults to
       :class:`~trieste.models.optimizer.Optimizer` with :class:`~gpflow.optimizers.Scipy`.
   :param use_natgrads: If True then alternate model optimization steps with natural
       gradient updates. Note that natural gradients requires
       an :class:`~trieste.models.optimizer.Optimizer` optimizer.
   :natgrad_gamma: Gamma parameter for the natural gradient optimizer.
   :raise ValueError (or InvalidArgumentError): If ``model``'s :attr:`q_sqrt` is not rank 3
       or if attempting to combine natural gradients with a :class:`~gpflow.optimizers.Scipy`
       optimizer.

   .. py:method:: model(self) -> gpflow.models.VGP
      :property:

      The underlying GPflow model.


   .. py:method:: update(self, dataset: trieste.data.Dataset, *, jitter: float = DEFAULTS.JITTER) -> None

      Update the model given the specified ``dataset``. Does not train the model.

      :param dataset: The data with which to update the model.
      :param jitter: The size of the jitter to use when stabilizing the Cholesky decomposition of
          the covariance matrix.


   .. py:method:: optimize(self, dataset: trieste.data.Dataset) -> None

      :class:`VariationalGaussianProcess` has a custom `optimize` method that (optionally) permits
      alternating between standard optimization steps (for kernel parameters) and natural gradient
      steps for the variational parameters (`q_mu` and `q_sqrt`). See :cite:`salimbeni2018natural`
      for details. Using natural gradients can dramatically speed up model fitting, especially for
      ill-conditioned posteriors.

      If using natural gradients, our optimizer inherits the mini-batch behavior and number
      of optimization steps as the base optimizer specified when initializing
      the :class:`VariationalGaussianProcess`.



.. py:data:: supported_models
   :annotation: :dict[Any, collections.abc.Callable[[Any, trieste.models.optimizer.Optimizer], TrainableProbabilisticModel]]

   A mapping of third-party model types to :class:`CustomTrainable` classes that wrap models of those
   types.


.. py:function:: randomize_hyperparameters(object: gpflow.Module) -> None

   Sets hyperparameters to random samples from their constrained domains or (if not constraints
   are available) their prior distributions.

   :param object: Any gpflow Module.


.. py:function:: squeeze_hyperparameters(object: gpflow.Module, alpha: float = 0.01, epsilon: float = 1e-07) -> None

   Squeezes the parameters to be strictly inside their range defined by the Sigmoid,
   or strictly greater than the limit defined by the Shift+Softplus.
   This avoids having Inf unconstrained values when the parameters are exactly at the boundary.

   :param object: Any gpflow Module.
   :param alpha: the proportion of the range with which to squeeze for the Sigmoid case
   :param epsilon: the value with which to offset the shift for the Softplus case.
   :raise ValueError: If ``alpha`` is not in (0,1) or epsilon <= 0


